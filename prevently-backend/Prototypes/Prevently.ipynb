{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Dataset extraction"
      ],
      "metadata": {
        "id": "IaiPluHhGQ9S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bea09vgI305b"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n",
        "\n",
        "API_KEY = \"b52a0ad46acf458f99cfe0ea084ce5a5\"\n",
        "query = \"economy OR politics OR technology\"\n",
        "\n",
        "all_articles = []\n",
        "for page in range(1, 6):\n",
        "    url = f\"https://newsapi.org/v2/everything?q={query}&language=en&pageSize=100&page={page}&apiKey={API_KEY}\"\n",
        "    response = requests.get(url).json()\n",
        "    if response.get(\"status\") == \"ok\" and \"articles\" in response:\n",
        "        all_articles.extend(response[\"articles\"])\n",
        "    else:\n",
        "        print(\"Eroare API:\", response.get(\"message\"))\n",
        "        break\n",
        "\n",
        "df = pd.DataFrame([\n",
        "    {\n",
        "        \"timestamp\": a[\"publishedAt\"],\n",
        "        \"title\": a[\"title\"],\n",
        "        \"description\": a[\"description\"],\n",
        "        \"content\": a[\"content\"],\n",
        "        \"source\": a[\"source\"][\"name\"],\n",
        "        \"source_url\": a[\"url\"]\n",
        "    }\n",
        "    for a in all_articles\n",
        "])\n",
        "df.to_csv(\"news_raw.csv\", index=False)\n",
        "print(f\"Am salvat {len(df)} articole în news_raw.csv ✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Functions for subinterval clasification and text cleaning"
      ],
      "metadata": {
        "id": "qPBNJ9Y-Gg6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def classify_subinterval(score):\n",
        "    if score <= -0.7:\n",
        "        return \"Panic\"\n",
        "    elif score <= -0.4:\n",
        "        return \"Risk\"\n",
        "    elif score <= -0.1:\n",
        "        return \"Mildly negative sentiment\"\n",
        "    elif score < 0.1:\n",
        "        return \"Stable outlook\"\n",
        "    elif score < 0.4:\n",
        "        return \"Mildly optimistic sentiment\"\n",
        "    elif score < 0.7:\n",
        "        return \"Growth\"\n",
        "    else:\n",
        "        return \"Strong confidence\"\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text(separator=\" \")\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[\\r\\n\\t]+\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"/[a-zA-Z]+/\", \" \", text)\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "FMIU6jWQ31vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Dataset Annotation"
      ],
      "metadata": {
        "id": "LriJk8nWGv6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"news_raw.csv\")\n",
        "\n",
        "df[\"content\"] = (df[\"content\"].fillna('')).apply(clean_text)\n",
        "\n",
        "tqdm.pandas()\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name)\n",
        "\n",
        "df[\"sentiment_result\"] = df[\"content\"].progress_apply(lambda x: sentiment_pipeline(x[:512])[0])\n",
        "\n",
        "label_to_value = {\n",
        "    \"1 star\": -1.0,\n",
        "    \"2 stars\": -0.5,\n",
        "    \"3 stars\": 0.0,\n",
        "    \"4 stars\": 0.5,\n",
        "    \"5 stars\": 1.0\n",
        "}\n",
        "\n",
        "df[\"sentiment_numeric\"] = df[\"sentiment_result\"].apply(lambda x: label_to_value[x[\"label\"]] * x[\"score\"])\n",
        "\n",
        "df[\"sentiment_sublabel\"] = df[\"sentiment_numeric\"].apply(classify_subinterval)\n",
        "\n",
        "df.to_csv(\"news_with_sentiment.csv\", index=False)\n",
        "print(df[[\"title\", \"sentiment_numeric\", \"sentiment_sublabel\"]].head())"
      ],
      "metadata": {
        "id": "iFjSr5Qp3432"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Company extraction from content"
      ],
      "metadata": {
        "id": "tXkWXE6iHJ7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert"
      ],
      "metadata": {
        "id": "RHKah77Zy6QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", grouped_entities=True)\n",
        "\n",
        "def extract_companies(text):\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return []\n",
        "    entities = ner_pipeline(text)\n",
        "    return [e['word'] for e in entities if e['entity_group'] == 'ORG']\n",
        "\n",
        "df[\"companies\"] = df[\"content\"].progress_apply(extract_companies)\n",
        "\n",
        "df.to_csv(\"news_with_sentiment_and_companies.csv\", index=False)"
      ],
      "metadata": {
        "id": "_Ohy0xw_1jgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Domain extraction"
      ],
      "metadata": {
        "id": "nHlgHG8K_s5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "df = pd.read_csv(\"news_with_sentiment_and_companies.csv\")\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "candidate_labels = [\n",
        "    \"technology\", \"finance\", \"healthcare\", \"energy\", \"industrials\", \"consumer_discretionary\",\n",
        "    \"materials\", \"communication_services\", \"consumer_staples\", \"utilities\", \"real_estate\"\n",
        "]\n",
        "\n",
        "def extract_domain(text):\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"Unknown\"\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    try:\n",
        "        result = classifier(text[:512], candidate_labels)\n",
        "        return result[\"labels\"][0]\n",
        "    except Exception as e:\n",
        "        print(\"EROARE clasificare domeniu:\", e)\n",
        "        return \"Unknown\"\n",
        "\n",
        "tqdm.pandas()\n",
        "df[\"domain\"] = df[\"content\"].progress_apply(extract_domain)\n",
        "\n",
        "df.to_csv(\"news_with_sentiment_companies_and_domain.csv\", index=False)\n",
        "print(\"✅ CSV final salvat cu domenii incluse!\")\n",
        "print(df[[\"title\", \"domain\", \"sentiment_sublabel\"]].head())\n"
      ],
      "metadata": {
        "id": "rCedBICc6TLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. JSON convert"
      ],
      "metadata": {
        "id": "9zTqnJowKWxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import uuid\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"news_with_sentiment_companies_and_domain.csv\")\n",
        "df[\"id\"] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
        "\n",
        "df_export = df.drop(columns=[\"content\"])\n",
        "\n",
        "df_export.to_json(\n",
        "    \"news_with_sentiment_companies_and_domain.json\",\n",
        "    orient=\"records\",\n",
        "    force_ascii=False,\n",
        "    indent=2\n",
        ")\n",
        "\n",
        "print(\"✅ Am salvat news_with_sentiment_companies_and_domain.json\")\n"
      ],
      "metadata": {
        "id": "ELZ4Jtd4yhLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Change domain spelling"
      ],
      "metadata": {
        "id": "KWrIiPMj6xAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"news_with_sentiment_companies_and_domain.json\")\n",
        "\n",
        "df[\"domain\"] = df[\"domain\"].str.lower().str.replace(\" \", \"_\")\n",
        "\n",
        "df.to_json(\n",
        "    \"news_with_sentiment_companies_and_domain.json\",\n",
        "    orient=\"records\",\n",
        "    force_ascii=False,\n",
        "    indent=2\n",
        ")\n",
        "\n",
        "print(\"✅ Domeniile au fost normalizate în JSON\")\n"
      ],
      "metadata": {
        "id": "vgtxqx1T6xXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Send json to Firestore Database"
      ],
      "metadata": {
        "id": "cITdLOmicrze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, firestore\n",
        "\n",
        "if not firebase_admin._apps:\n",
        "    cred = credentials.Certificate(\"prevently-cdae1-firebase-adminsdk-fbsvc-2686544549.json\")\n",
        "    firebase_admin.initialize_app(cred)\n",
        "\n",
        "db = firestore.client()\n",
        "\n",
        "with open(\"news_timestamped.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "collection_name = \"news_datastore\"\n",
        "\n",
        "for item in data:\n",
        "    doc_id = str(item.get(\"id\")) if \"id\" in item else None\n",
        "\n",
        "    if doc_id:\n",
        "        db.collection(collection_name).document(doc_id).set(item)\n",
        "    else:\n",
        "        db.collection(collection_name).add(item)\n",
        "\n",
        "print(f\"✅ Am încărcat {len(data)} articole în colecția '{collection_name}' din Firestore\")\n"
      ],
      "metadata": {
        "id": "86HR03D3R3T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Sentiment prediction - function"
      ],
      "metadata": {
        "id": "IbEuTDsQfA7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet import Prophet\n",
        "import pandas as pd\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "def predict_sentiment_future(timestamps, sentiment_scores, periods=7):\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"ds\": pd.to_datetime(timestamps, utc=True).tz_localize(None),\n",
        "        \"y\": sentiment_scores\n",
        "    })\n",
        "\n",
        "    model = Prophet()\n",
        "    model.fit(df)\n",
        "\n",
        "    future = model.make_future_dataframe(periods=periods)\n",
        "    forecast = model.predict(future)\n",
        "\n",
        "    predicted = forecast.tail(periods)[[\"ds\", \"yhat\"]]\n",
        "\n",
        "    return {\n",
        "        \"future_dates\": predicted[\"ds\"].dt.strftime(\"%Y-%m-%d\").tolist(),\n",
        "        \"predicted_scores\": predicted[\"yhat\"].tolist()\n",
        "    }\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class SentimentInput(BaseModel):\n",
        "    timestamps: list[str]\n",
        "    sentiment_scores: list[float]\n",
        "    periods: int = 7\n",
        "\n",
        "@app.post(\"/predict_sentiment\")\n",
        "def predict_sentiment(data: SentimentInput):\n",
        "    return predict_sentiment_future(\n",
        "        data.timestamps,\n",
        "        data.sentiment_scores,\n",
        "        data.periods\n",
        "    )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RaXdV4abQ5G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet import Prophet\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"news_with_sentiment_and_companies.csv\")\n",
        "\n",
        "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True).dt.tz_localize(None)\n",
        "df = df.sort_values(\"timestamp\")\n",
        "\n",
        "df_prophet = df[[\"timestamp\", \"sentiment_numeric\"]].rename(columns={\"timestamp\": \"ds\", \"sentiment_numeric\": \"y\"})\n",
        "\n",
        "model = Prophet()\n",
        "model.fit(df_prophet)\n",
        "\n",
        "future = model.make_future_dataframe(periods=7)\n",
        "forecast = model.predict(future)\n",
        "\n",
        "model.plot(forecast)\n",
        "model.plot_components(forecast)\n",
        "\n"
      ],
      "metadata": {
        "id": "U1nOAgaFdSBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. General tags extract"
      ],
      "metadata": {
        "id": "UPskiLC3-9Sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT\n",
        "\n",
        "print(\"🔍 Extragem tag-uri din content...\")\n",
        "\n",
        "tag_model_name = \"ml6team/keyphrase-extraction-distilbert-inspec\"\n",
        "kw_model = KeyBERT(model=tag_model_name)\n",
        "\n",
        "def extract_tags(text):\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return []\n",
        "    text = clean_text(text)\n",
        "    text = text[:1000]\n",
        "    try:\n",
        "        keywords = kw_model.extract_keywords(\n",
        "            text,\n",
        "            keyphrase_ngram_range=(1, 3),\n",
        "            stop_words=\"english\",\n",
        "            top_n=5\n",
        "        )\n",
        "        tags = [kw for kw, score in keywords]\n",
        "        return tags\n",
        "    except Exception as e:\n",
        "        print(\"Eroare la extragerea tag-urilor:\", e)\n",
        "        return []\n",
        "\n",
        "df[\"tags\"] = df[\"content\"].progress_apply(extract_tags)\n"
      ],
      "metadata": {
        "id": "Dr0eIffoHJFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Timestamp conversion"
      ],
      "metadata": {
        "id": "n-uuR4oXYQOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "with open(\"news_with_sentiment_companies_and_domain.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "for item in data:\n",
        "    ts = item.get(\"timestamp\")\n",
        "    if isinstance(ts, (int, float)):\n",
        "        item[\"timestamp\"] = datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "\n",
        "with open(\"news_timestamped.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✅ Timestamp numeric transformat în ISO 8601 în JSON\")\n"
      ],
      "metadata": {
        "id": "XKRmhDA-CVfS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}